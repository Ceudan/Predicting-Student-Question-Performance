{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Contest Ensemble","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM0l6dl0KHwpH2dQug8vHDy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#torch, pandas\n","from torch import autograd\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data\n","import pandas as pd\n","\n","#numpy, matlab, file formats\n","import numpy as np\n","import math\n","import warnings\n","import matplotlib.pyplot as plt\n","import csv\n","\n","#project programs\n","from utils import *\n","from knn import knn_impute_by_user\n","from item_response import irt, sigmoid\n","from neural_network import AutoEncoder, train\n","from sklearn.impute import KNNImputer\n","\n","import random"],"metadata":{"id":"aMxocgwOtBKa","executionInfo":{"status":"ok","timestamp":1641335325507,"user_tz":300,"elapsed":279,"user":{"displayName":"tigerfan707","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17162744304693628162"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["#Load all data\n","path = \"/content/\"\n","train_data = load_train_csv(path)\n","sparse_matrix = load_train_sparse(path).toarray()\n","val_data = load_valid_csv(path)\n","private_test = load_private_test_csv(path)\n","test_data = load_public_test_csv(path)"],"metadata":{"id":"ob7SHHVcAIW2","executionInfo":{"status":"ok","timestamp":1641328388669,"user_tz":300,"elapsed":289,"user":{"displayName":"tigerfan707","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17162744304693628162"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def sparse_matrix_evaluate(data, matrix, threshold=0.5):\n","    \"\"\" Given the sparse matrix represent, return the accuracy of the prediction on data.\n","    :param data: A dictionary {user_id: list, question_id: list, is_correct: list}\n","    :param matrix: 2D matrix\n","    :param threshold: float\n","    :return: float\n","    \"\"\"\n","    total_prediction = 0\n","    total_accurate = 0\n","    preds = []\n","    for i in range(len(data[\"user_id\"])):\n","        cur_user_id = data[\"user_id\"][i]\n","        cur_question_id = data[\"question_id\"][i]\n","        #store float predictions of user\n","        preds.append(matrix[cur_user_id, cur_question_id])\n","\n","        #Some datasets don't have a label. Skip accuracy calculation step for those.\n","        if(len(data[\"is_correct\"])!=0):\n","          if matrix[cur_user_id, cur_question_id] >= threshold and data[\"is_correct\"][i]:\n","              total_accurate += 1\n","          if matrix[cur_user_id, cur_question_id] < threshold and not data[\"is_correct\"][i]:\n","              total_accurate += 1\n","        total_prediction += 1\n","\n","    acc = total_accurate/total_prediction\n","\n","    return acc, preds"],"metadata":{"id":"oERODzjaa3GN","executionInfo":{"status":"ok","timestamp":1641329593447,"user_tz":300,"elapsed":424,"user":{"displayName":"tigerfan707","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17162744304693628162"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["def evaluate_ir(data, theta, beta):\n","    '''\n","    :param data: A dictionary {user_id: list, question_id: list,\n","    is_correct: list}\n","\n","    :param theta: Vector\n","    :param beta: Vector\n","    :return: float\n","    '''\n","    pred = []\n","    preds = []\n","\n","    for i, q in enumerate(data[\"question_id\"]):\n","        u = data[\"user_id\"][i]\n","        x = (theta[u] - beta[q]).sum()\n","        p_a = sigmoid(x)\n","        #Store the decimal predictions of model in list\n","        preds.append(p_a)\n","        #binary prediction used for accuracy calculations\n","        pred.append(p_a >= 0.5)\n","    \n","    #Some datasets don't have a label. Skip accuracy calculation step for those.\n","    if(len(data[\"is_correct\"])!=0):\n","      acc = np.sum((data[\"is_correct\"] == np.array(pred))) \\\n","          / len(data[\"is_correct\"])\n","    else:\n","      acc = 0\n","\n","    return acc, preds"],"metadata":{"id":"ydUDy-eCY5uV","executionInfo":{"status":"ok","timestamp":1641329596390,"user_tz":300,"elapsed":281,"user":{"displayName":"tigerfan707","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17162744304693628162"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["def evaluate_nn(model, train_data, data):\n","    '''\n","    :param train_data: 2D FloatTensor\n","    :param valid_data: A dictionary {user_id: list, question_id: list, is_correct: list}\n","    '''\n","    # Tell PyTorch you are evaluating the model.\n","    model.eval()\n","\n","    total = 0\n","    correct = 0\n","\n","    preds = []\n","\n","    for i, u in enumerate(data[\"user_id\"]):\n","        inputs = Variable(train_data[u]).unsqueeze(0)\n","        output = model(inputs)\n","\n","        guess = output[0][data[\"question_id\"][i]].item()\n","        #Add the prediction of each valid question\n","        preds.append(guess)\n","        guess = guess>= 0.5\n","\n","        #Some datasets don't have a label. Skip accuracy calculation step for those.\n","        if(len(data[\"is_correct\"])!=0):\n","          if guess == data[\"is_correct\"][i]:\n","              correct += 1\n","        total += 1\n","    acc = correct /float(total)\n","    \n","    return acc, preds"],"metadata":{"id":"lovZs1PbExxN","executionInfo":{"status":"ok","timestamp":1641329598304,"user_tz":300,"elapsed":3,"user":{"displayName":"tigerfan707","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17162744304693628162"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["def _get_user_id(question_id, test_data=test_data):\n","    # Only works when question is in test dataset.\n","    df = pd.DataFrame.from_dict(test_data)\n","    q = df[df[\"question_id\"] == question_id]\n","    return q.user_id\n","\n","\n","def _get_is_correct(question_id, test_data=test_data):\n","    # Only works when question is in test dataset.\n","    df = pd.DataFrame.from_dict(test_data)\n","    q = df[df[\"question_id\"] == question_id]\n","    p_a = np.sum(q.is_correct) / len(q.is_correct)\n","    if p_a > 0.5:\n","        truth = True\n","    else:\n","        truth = False\n","    return truth"],"metadata":{"id":"MaKiCYwYFOKt","executionInfo":{"status":"ok","timestamp":1641328469784,"user_tz":300,"elapsed":277,"user":{"displayName":"tigerfan707","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17162744304693628162"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# knn_impute_by_item\n","# Hyperparameters: k\n","def KNNU(sparse_matrix, valid_data, test_data, private_test, k=11):\n","    nbrs = KNNImputer(n_neighbors=k)\n","    mat = nbrs.fit_transform(sparse_matrix)\n","    acc,p1 = sparse_matrix_evaluate(valid_data, mat)\n","    _,p2 = sparse_matrix_evaluate(test_data,mat)\n","    _,p3 = sparse_matrix_evaluate(private_test, mat)\n","    print(acc)\n","\n","    return p1,p2,p3\n","\n","# knn_impute_by_item\n","# Hyperparameters: k\n","def KNNI(sparse_matrix, valid_data, test_data, private_test, k=21):\n","    nbrs = KNNImputer(n_neighbors=k)\n","    mat = nbrs.fit_transform(sparse_matrix.T)\n","    acc,p1 = sparse_matrix_evaluate(valid_data, mat.T)\n","    _,p2 = sparse_matrix_evaluate(test_data, mat.T)\n","    _,p3 = sparse_matrix_evaluate(private_test, mat.T)\n","    print(acc)\n","\n","    return p1,p2,p3\n","\n","\n","# item_response\n","# Hyperparameters: lr, iterations\n","def ITR(train_data, val_data, test_data, private_test, lr=0.03, iterations=12):\n","    theta, beta, val_acc_lst, neg_lld_lst = irt(train_data, val_data, lr, iterations)\n","\n","    _,p1 = evaluate_ir(val_data, theta, beta)\n","    _,p2 = evaluate_ir(test_data, theta, beta)\n","    _,p3 = evaluate_ir(private_test, theta, beta)\n","    return p1,p2,p3\n","\n","\n","# neural_network\n","# Hyperameters: layer sizes, lr, num_epoch, lamb\n","def NNet(sparse_matrix, val_data, test_data, private_test, lr=0.005, num_epoch=15, lamb=0.01):\n","    train_matrix = torch.FloatTensor(sparse_matrix.copy())\n","    zero_train_matrix = torch.FloatTensor(sparse_matrix.copy())\n","\n","    with warnings.catch_warnings():\n","        warnings.simplefilter(\"ignore\")\n","        zero_train_matrix[np.isnan(train_matrix)] = 0\n","\n","    zero_train_matrix = torch.FloatTensor(zero_train_matrix)\n","    #Layer Sizes are defined here\n","    model = AutoEncoder(1774,400,100)\n","    train(model, lr, lamb, train_matrix, zero_train_matrix, val_data, num_epoch)\n","    model.eval()\n","\n","    _,p1 = evaluate_nn(model, zero_train_matrix, val_data)\n","    _,p2 = evaluate_nn(model, zero_train_matrix, test_data)\n","    _,p3 = evaluate_nn(model, zero_train_matrix, private_test)\n","    return p1,p2,p3"],"metadata":{"id":"NIm70zm7FLfV","executionInfo":{"status":"ok","timestamp":1641329649074,"user_tz":300,"elapsed":272,"user":{"displayName":"tigerfan707","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17162744304693628162"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","execution_count":26,"metadata":{"id":"urW_OChvdnUp","executionInfo":{"status":"ok","timestamp":1641328485796,"user_tz":300,"elapsed":333,"user":{"displayName":"tigerfan707","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17162744304693628162"}}},"outputs":[],"source":["def single_run(*args):\n","    # Bagging\n","    # Trim dataset and update bagged\n","    #IRTT Uses a random ?%\n","    df = pd.DataFrame.from_dict(train_data).sample(n=int(len(train_data[\"question_id\"])*1))  \n","    bagged2 = {\"user_id\": list(df.user_id), \"question_id\": list(df.question_id), \"is_correct\": list(df.is_correct)}\n","\n","    mat1 = sparse_matrix.copy()\n","    mat2 = sparse_matrix.copy()\n","\n","    # Randomly select indicies\n","    #KNN Uses a Random (1-?)%\n","    indicies1 = np.random.choice(mat1.shape[1]*mat1.shape[0], replace=False, size=int(mat1.shape[1]*mat1.shape[0]*0))\n","    #ANN Uses a random (1-?)%\n","    indicies2 = np.random.choice(mat2.shape[1]*mat2.shape[0], replace=False, size=int(mat2.shape[1]*mat2.shape[0]*0))\n","\n","    # Trim dataset by indicies\n","    # Cite: https://stackoverflow.com/questions/48536969/how-to-randomly-set-elements-in-numpy-array-to-0\n","    mat1[np.unravel_index(indicies1, mat1.shape)] = None  # Use a tenth of dataset (random)\n","    mat2[np.unravel_index(indicies2, mat2.shape)] = None  # Use a tenth of dataset (random)\n","\n","    # Set bagged\n","    bagged1 = mat1\n","    bagged3 = mat2\n","\n","\n","    # Run machine learning\n","    print(\"[STATUS] Running KNN...\")\n","    #predictions for each model, for each dataset (valid,test,private_test)\n","    kup1,kup2,kup3 = KNNU(bagged1,val_data,test_data, private_test)\n","    kip1,kip2,kip3 = KNNI(bagged1,val_data,test_data, private_test)\n","    print(\"[STATUS] Running IRT...\")\n","    ip1,ip2,ip3 = ITR(bagged2, val_data,test_data, private_test)\n","    print(\"[STATUS] Running NNet...\")\n","    np1,np2,np3 = NNet(bagged3, val_data, test_data, private_test)\n","\n","    print(\"[STATUS] Completed!\")\n","    \n","    #These are the predictions for each model (across dimension 1), for each dataset (acorss dimenion 0)\n","    return [kup1,kip1,ip1,np1],[kup2,kip2,ip2,np2],[kup3,kip3,ip3,np3]\n","\n"]},{"cell_type":"code","source":["#We are going to average the predictions of each algorithm with the given weights\n","def evaluate_preds(ap,data,name,weight):\n","  \n","  #Get weighted average\n","  ap = np.array(ap)\n","  ap = np.mean(ap,axis=0)\n","  ap = np.average(ap,axis=0,weights = weight)\n","\n","  #Get binary prediction as True or False\n","  bp = ap>=0.5\n","  acc = \"N/A\"\n","  if(name!=\"Private_Test\"):\n","      acc = np.sum((data[\"is_correct\"] == bp)/ len(data[\"is_correct\"]))\n","      iter = len(ap)\n","      print(\"\\n\\n Iteration\",iter,\":Bagged\",name,\"Accuracy  = \",acc,\"\\n\\n\")\n","\n","  #Get binary prediction as 1.0 or 0.0\n","  bp = []\n","  for item in ap:\n","    if(item>=0.5):\n","      bp.append(1.)\n","    else:\n","      bp.append(0.)\n","\n","  return bp,acc"],"metadata":{"id":"2ggqNHrYCtWR","executionInfo":{"status":"ok","timestamp":1641334929543,"user_tz":300,"elapsed":272,"user":{"displayName":"tigerfan707","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17162744304693628162"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["#We will use pytorch optimization to find the best weight for each algorithm.\n","def get_best_weights(ap1,ap2,lr):\n","  #Choose a good initial geuss\n","  weights = torch.tensor([0.1380, 0.4688, 1.8051, 0.7092],dtype=torch.float64, requires_grad = True)\n","  optimizer = torch.optim.SGD([weights], lr = lr)\n","\n","  #convert data to tensors\n","  val_labels = torch.tensor(val_data[\"is_correct\"])\n","\n","  ap1 = np.array(ap1)\n","  ap1 = np.mean(ap1,axis=0)\n","  ap1 = torch.tensor(ap1)\n","  ap1 = torch.transpose(ap1,0,1)\n","  print(ap1.shape)\n","\n","  for i in range(0,100000):\n","    print(\"WEIGHTS:\",weights)\n","    output = torch.matmul(ap1,weights)/torch.sum(weights)\n","\n","    loss = torch.sum((output - val_labels) ** 2.)/7086\n","    print(\"LOSS:\",loss)\n","    loss.backward()\n","    print(\"GRADIENT:\",weights.grad)\n","    optimizer.step()\n","    optimizer.zero_grad()"],"metadata":{"id":"Kj82RJ2gtOu6","executionInfo":{"status":"ok","timestamp":1641334932319,"user_tz":300,"elapsed":264,"user":{"displayName":"tigerfan707","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17162744304693628162"}}},"execution_count":89,"outputs":[]},{"cell_type":"code","source":["'''Following lists store predictions for each model (dim 0), for each bag (dim 1). 3 lists for each dataset'''\n","'''DO NOT RUN BY ACCIDENT. WILL CLEAR ALL BAGGING PROGRESS'''\n","#ap1 = []\n","#ap2 = []\n","#ap3 = []"],"metadata":{"id":"dlXoEWFDGX4h","executionInfo":{"status":"ok","timestamp":1641334934452,"user_tz":300,"elapsed":3,"user":{"displayName":"tigerfan707","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17162744304693628162"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","source":["'''Predictions are generated here.'''\n","'''You can stop the cell at any point and return later, bagging progress is saved inside varaibles'''\n","if __name__ == \"__main__\":\n","  #Number of Bag iterations\n","  iters = 100\n","  for i in range(iters):\n","      preds1,preds2,preds3 = single_run()\n","      ap1.append(preds1)\n","      ap2.append(preds2)\n","      ap3.append(preds3)\n","\n","      #Evaluate current list of all bagged predictions\n","      weights= [0.1380, 0.4688, 1.8051, 0.7092]\n","      evaluate_preds(ap1, val_data,\"Valid_Data\", weights)\n","      evaluate_preds(ap2, test_data,\"Test_Data\", weights)\n","      evaluate_preds(ap3, private_test,\"Private_Test\", weights)"],"metadata":{"id":"3Fg4V0RgFT_l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Find the best weights per model for ensemble\n","get_best_weights(ap1,ap2,1)"],"metadata":{"id":"t7DX-owDuX1h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weights = [0.1380, 0.4688, 1.8051, 0.7092]\n","\n","#Observe your acccuracy on the valid and test data sets\n","evaluate_preds(ap1,val_data,\"Valid_Data\",weights)\n","evaluate_preds(ap2,test_data,\"Test_Data\",weights)\n","\n","#bp3 are your binary predictions for the private_test data (dataset 3).\n","bp3,na = evaluate_preds(ap3,private_test,\"Private_Test\",weights)"],"metadata":{"id":"DJZE_LFJg3nl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save predictions for private_test data for the contest.\n","private_test[\"is_correct\"] = bp3\n","save_private_test_csv(private_test)"],"metadata":{"id":"6G5PYE8zOb1x","executionInfo":{"status":"ok","timestamp":1641330922547,"user_tz":300,"elapsed":441,"user":{"displayName":"tigerfan707","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17162744304693628162"}}},"execution_count":84,"outputs":[]}]}